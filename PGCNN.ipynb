{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 6890 * 1\n",
    "n_hidden=16   # check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = h5py.File('aaa11.mat')\n",
    "data2 = h5py.File('aaa22.mat')\n",
    "data3 = h5py.File('aaa33.mat')\n",
    "data4 = h5py.File('aaa44.mat')\n",
    "data5 = h5py.File('aaa55.mat')\n",
    "data6 = h5py.File('aaa66.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data11=list(data1[\"desc\"])\n",
    "data22=list(data2[\"desc\"])\n",
    "data33=list(data3[\"desc\"])\n",
    "data44=list(data4[\"desc\"])\n",
    "data55=list(data5[\"desc\"])\n",
    "data66=list(data6[\"desc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 6890)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(data11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "data11=np.array(list(itertools.zip_longest(*data11, fillvalue=0))).T\n",
    "\n",
    "data22=np.array(list(itertools.zip_longest(*data22, fillvalue=0))).T\n",
    "\n",
    "data33=np.array(list(itertools.zip_longest(*data33, fillvalue=0))).T\n",
    "\n",
    "data44=np.array(list(itertools.zip_longest(*data44, fillvalue=0))).T\n",
    "\n",
    "data55=np.array(list(itertools.zip_longest(*data55, fillvalue=0))).T\n",
    "\n",
    "data66=np.array(list(itertools.zip_longest(*data66, fillvalue=0))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Realdata = np.vstack([data11,data22,data33,data44,data55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08384416, 0.0157384 , 0.01330916, ..., 0.00544532, 0.00180431,\n",
       "        0.00010812],\n",
       "       [0.08228676, 0.01492653, 0.01200653, ..., 0.01331328, 0.01662881,\n",
       "        0.00468025],\n",
       "       [0.07465372, 0.01346919, 0.01074188, ..., 0.00686487, 0.00733826,\n",
       "        0.00228694],\n",
       "       ...,\n",
       "       [0.04929596, 0.00738491, 0.0042837 , ..., 0.00877364, 0.00757886,\n",
       "        0.00084227],\n",
       "       [0.05034327, 0.00757916, 0.00443882, ..., 0.00842075, 0.00724801,\n",
       "        0.0008136 ],\n",
       "       [0.05089838, 0.00764184, 0.00445396, ..., 0.00762891, 0.00678711,\n",
       "        0.00088101]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(Realdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 6890)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Realdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "realdata=np.transpose(Realdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6890, 750)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(realdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "realdata1= tf.convert_to_tensor(realdata, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(6890), Dimension(750)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realdata1.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1 = h5py.File('d11.mat')\n",
    "filter2 = h5py.File('d22.mat')\n",
    "filter3 = h5py.File('d33.mat')\n",
    "filter4 = h5py.File('d44.mat')\n",
    "filter5 = h5py.File('d55.mat')\n",
    "filter6 = h5py.File('d66.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter11=list(filter1[\"M\"])\n",
    "filter22=list(filter2[\"M\"])\n",
    "filter33=list(filter3[\"M\"])\n",
    "filter44=list(filter4[\"M\"])\n",
    "filter55=list(filter5[\"M\"])\n",
    "filter66=list(filter6[\"M\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1=csr_matrix((filter1[\"M\"][\"data\"],filter1[\"M\"][\"ir\"],filter1[\"M\"][\"jc\"]), dtype=np.float32)\n",
    "\n",
    "f2=csr_matrix((filter2[\"M\"][\"data\"],filter2[\"M\"][\"ir\"],filter2[\"M\"][\"jc\"]), dtype=np.float32)\n",
    "\n",
    "f3=csr_matrix((filter3[\"M\"][\"data\"],filter3[\"M\"][\"ir\"],filter3[\"M\"][\"jc\"]), dtype=np.float32)\n",
    "\n",
    "f4=csr_matrix((filter4[\"M\"][\"data\"],filter4[\"M\"][\"ir\"],filter4[\"M\"][\"jc\"]), dtype=np.float32)\n",
    "\n",
    "f5=csr_matrix((filter5[\"M\"][\"data\"],filter5[\"M\"][\"ir\"],filter5[\"M\"][\"jc\"]), dtype=np.float32)\n",
    "\n",
    "f6=csr_matrix((filter6[\"M\"][\"data\"],filter6[\"M\"][\"ir\"],filter6[\"M\"][\"jc\"]), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "f11=np.transpose(f1)\n",
    "f22=np.transpose(f2)\n",
    "f33=np.transpose(f3)\n",
    "f44=np.transpose(f4)\n",
    "f55=np.transpose(f5)\n",
    "f66=np.transpose(f6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(551200, 6890)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(f11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMST(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape) # 의미가 없을수도 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = SMST(f11)\n",
    "T2 = SMST(f22)\n",
    "T3 = SMST(f33)\n",
    "T4 = SMST(f44)\n",
    "T5 = SMST(f55)\n",
    "T6 = SMST(f66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x18a072f2828>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(T1, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'SparseTensorDenseMatMul_1/SparseTensorDenseMatMul:0' shape=(551200, 750) dtype=float32>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse_tensor_dense_matmul(T1,realdata1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import generators, division, absolute_import, with_statement, print_function, unicode_literals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = np.array(range(150))\n",
    "D2 = np.array(range(151,300))\n",
    "D3 = np.array(range(301,450))\n",
    "D4 = np.array(range(451,600))\n",
    "D5 = np.array(range(601,750))\n",
    "D6 = np.array(range(751,900))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763,\n",
       "       764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776,\n",
       "       777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789,\n",
       "       790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802,\n",
       "       803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815,\n",
       "       816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828,\n",
       "       829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841,\n",
       "       842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854,\n",
       "       855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867,\n",
       "       868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880,\n",
       "       881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893,\n",
       "       894, 895, 896, 897, 898, 899])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-43-bc28c553892a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-43-bc28c553892a>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    with tf.name_scope(\"contrastive-loss\"):\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"contrastive-loss\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_train_label = np.array([1,2,3,4,5])\n",
    "map_train_label_indices = {'1':D1,'2':D2,'3':D3,'4':D4,'5':D5}\n",
    "    \n",
    "def _get_siamese_similar_pair(self):\n",
    "    label =np.random.choice(unique_train_label)\n",
    "    l, r = np.random.choice(map_train_label_indices[str(label)], 2, replace=False)\n",
    "    \n",
    "    return l, r, 1\n",
    "\n",
    "def _get_siamese_dissimilar_pair(self):\n",
    "    label_l, label_r = np.random.choice(unique_train_label, 2, replace=False)\n",
    "\n",
    "    l = np.random.choice(map_train_label_indices[str(label_l)])\n",
    "\n",
    "    r = np.random.choice(map_train_label_indices[str(label_r)])\n",
    "    \n",
    "    return l, r, 0\n",
    "    \n",
    "def _get_siamese_pair(self):\n",
    "    if np.random.random() < 0.5:\n",
    "        return _get_siamese_similar_pair(self)\n",
    "    else:\n",
    "        return _get_siamese_dissimilar_pair(self)\n",
    "        \n",
    "def get_siamese_batch(n_batch,self):\n",
    "    idxs_left, idxs_right, labels = [], [], []\n",
    "    for i in range(n_batch):\n",
    "        l, r, x = _get_siamese_pair(self)\n",
    "        idxs_left.append(l)\n",
    "        idxs_right.append(r)\n",
    "        labels.append(x)\n",
    "    return Realdata[idxs_left,:], Realdata[idxs_right,:], np.expand_dims(labels, axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-d16df770e7b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "tf.convert_to_tensor(A, dtype=tf.float32), tf.convert_to_tensor(B, dtype=tf.float32 ), C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fildic = {'1':T1,'2':T2,'3':T3,'4':T4,'5':T5,'6':T6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 50\n",
    "\n",
    "def neuron_layer(X, n_neurons):\n",
    "    stddev = 2 / np.sqrt(batch_size+n_neurons)\n",
    "    init = tf.truncated_normal((n_neurons, batch_size), stddev=stddev)\n",
    "    W = tf.Variable(init, dtype=tf.float32, name=\"kernel\")\n",
    "    Z = tf.matmul(W,X) \n",
    "    return tf.nn.relu(Z)\n",
    "    \n",
    "def model(X):\n",
    "    X = tf.placeholder(tf.float32,  shape=[ batch_size , n_inputs ], name='left')\n",
    "    Out = neuron_layer(X, 16)\n",
    "    Out1= tf.transpose(Out)\n",
    "    label = np.random.choice(unique_train_label)\n",
    "    B = fildic[str(label)]\n",
    "    for i in range( batch_size - 1 ):\n",
    "        L = np.random.choice(unique_train_label)\n",
    "        B = tf.sparse.add( B , fildic[str(L)] )\n",
    "    Out2 = tf.sparse_tensor_dense_matmul(B,Out1)\n",
    "    h , w = int(Out2.get_shape()[0]), int(Out2.get_shape()[1])\n",
    "    Out3 = tf.reshape(Out2, shape =[-1, h, w, -1])\n",
    "    return tf.layers.average_pooling2d(Out3,(80,1),(80,1),padding='valid')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def contrastive_loss(model1, model2, y, margin):\n",
    "    with tf.name_scope(\"contrastive-loss\"):\n",
    "        distance = tf.sqrt(tf.reduce_sum(tf.pow(model1 - model2, 2), 1, keepdims=True))\n",
    "        \n",
    "        similarity = y * tf.square(distance) \n",
    "        dissimilarity = (1 - y) * tf.square(tf.maximum((margin - distance), 0))  \n",
    "        return tf.reduce_mean(dissimilarity + similarity) / 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "left = tf.placeholder(tf.float32,  shape=[batch_size , n_inputs], name='left')\n",
    "right = tf.placeholder(tf.float32,  shape=[ batch_size , n_inputs], name='right')\n",
    "\n",
    "with tf.name_scope(\"similarity\"):\n",
    "    label = tf.placeholder(tf.int32, [1, None], name='label') # 1 if same, 0 if different\n",
    "    label_float = tf.to_float(label)\n",
    "    \n",
    "margin = 0.5\n",
    "left_output = model(left)\n",
    "right_output = model(right)\n",
    "loss = contrastive_loss(left_output, right_output, label_float, margin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\envs\\cuda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    starter_learning_rate = 0.0001\n",
    "    learning_rate = 0.0001\n",
    "    \n",
    "    tf.summary.scalar('lr', learning_rate)\n",
    "    \n",
    "    train_step = tf.train.AdadeltaOptimizer(learning_rate=0.0001, rho=0.96, epsilon=1e-08).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        batch_left, batch_right, batch_similarity = get_siamese_batch(batch_size, margin)\n",
    "        _, l = sess.run([train_step, loss], feed_dict={left:batch_left, right:batch_right, label: batch_similarity})\n",
    "            \n",
    "        print(\"\\r#%d - Loss\"%i, l)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\t\tfor i in range(FLAGS.train_iter):\n",
    "\t\t\tbatch_left, batch_right, batch_similarity = next_batch(FLAGS.batch_size)\n",
    "\n",
    "\t\t\t_, l, summary_str = sess.run([train_step, loss, merged],\n",
    "\t\t\t\t\t\t\t\t\t\t feed_dict={left:batch_left, right:batch_right, label: batch_similarity})\n",
    "\t\t\t\n",
    "\t\t\twriter.add_summary(summary_str, i)\n",
    "\t\t\tprint(\"\\r#%d - Loss\"%i, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        if (n_epochs + 1) % epoch == 0:\n",
    "            acc_test = sess.run(left_output, feed_dict={left: image_test})\n",
    "            print(\"테스트 세트에서 손실도:\" ,i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
